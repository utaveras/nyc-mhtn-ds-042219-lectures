{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running an XGboosted Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T19:24:58.470365Z",
     "start_time": "2019-06-25T19:24:56.487786Z"
    }
   },
   "outputs": [],
   "source": [
    "#import the appropriate packages\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, f1_score, roc_auc_score\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T19:24:58.531726Z",
     "start_time": "2019-06-25T19:24:58.473990Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read in data and split data to be used in the models\n",
    "titanic = pd.read_csv('cleaned_titanic.csv', index_col='PassengerId')\n",
    "\n",
    "# Create matrix of features\n",
    "X = titanic.drop('Survived', axis = 1) # grabs everything else but 'Survived'\n",
    "\n",
    "# Create target variable\n",
    "y = titanic['Survived'] # y is the column we're trying to predict\n",
    "\n",
    "# Create a list of the features being used in the \n",
    "feature_cols = X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T19:24:58.571636Z",
     "start_time": "2019-06-25T19:24:58.541193Z"
    }
   },
   "outputs": [],
   "source": [
    "# setting up testing and training sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=23)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost's hyperparameters\n",
    "\n",
    "At this point, before building the model, you should be aware of the tuning parameters that XGBoost provides. Well, there are a plethora of tuning parameters for tree-based learners in XGBoost and you can read all about them [here](https://xgboost.readthedocs.io/en/latest/parameter.html#general-parameters). But the most common ones that you should know are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall parameters have been divided into 3 categories by XGBoost authors:\n",
    "\n",
    "- **General Parameters:** Guide the overall functioning\n",
    "- **Booster Parameters:** Guide the individual booster (tree/regression) at each step\n",
    "- **Learning Task Parameters:** Guide the optimization performed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General Parameters\n",
    "These define the overall functionality of XGBoost.\n",
    "\n",
    "- **booster** [default=gbtree]\n",
    "Select the type of model to run at each iteration. It has 2 options:\n",
    "    - gbtree: tree-based models\n",
    "    - gblinear: linear models\n",
    "    \n",
    "- **silent** [default=0]:\n",
    "Silent mode is activated is set to 1, i.e. no running messages will be printed. It’s generally good to keep it 0 as the messages might help in understanding the model.\n",
    "\n",
    "- **nthread**  [default to maximum number of threads available if not set]\n",
    "This is used for parallel processing and number of cores in the system should be entered. If you wish to run on all cores, value should not be entered and algorithm will detect automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Booster Parameters\n",
    "Though there are 2 types of boosters, we’ll consider only tree booster here because it always outperforms the linear booster and thus the later is rarely used.\n",
    "\n",
    "- **eta [default=0.3]**\n",
    "    - Analogous to learning rate in GBM\n",
    "    - Makes the model more robust by shrinking the weights on each step\n",
    "    - Typical final values to be used: 0.01-0.2\n",
    "- **min_child_weight [default=1]**\n",
    "    - Defines the minimum sum of weights of all observations required in a child.\n",
    "    - This is similar to min_child_leaf in GBM but not exactly. This refers to min “sum of weights” of observations while GBM has min “number of observations”.\n",
    "    - Used to control over-fitting. Higher values prevent a model from learning relations which might be highly specific to the particular sample selected for a tree.\n",
    "    - Too high values can lead to under-fitting hence, it should be tuned using CV.\n",
    "- **max_depth [default=6]**\n",
    "    - The maximum depth of a tree, same as GBM.\n",
    "    - Used to control over-fitting as higher depth will allow model to learn relations very specific to a particular sample.\n",
    "    - Should be tuned using CV.\n",
    "    - Typical values: 3-10\n",
    "- **max_leaf_nodes**\n",
    "    - The maximum number of terminal nodes or leaves in a tree.\n",
    "    - Can be defined in place of max_depth. Since binary trees are created, a depth of ‘n’ would produce a maximum of 2^n leaves.\n",
    "    - If this is defined, GBM will ignore max_depth.\n",
    "- **gamma [default=0]**\n",
    "    - A node is split only when the resulting split gives a positive reduction in the loss function. Gamma specifies the minimum loss reduction required to make a split.\n",
    "    - Makes the algorithm conservative. The values can vary depending on the loss function and should be tuned.\n",
    "- **max_delta_step [default=0]**\n",
    "    - In maximum delta step we allow each tree’s weight estimation to be. If the value is set to 0, it means there is no constraint. If it is set to a positive value, it can help making the update step more conservative.\n",
    "    - Usually this parameter is not needed, but it might help in logistic regression when class is extremely imbalanced.\n",
    "    - This is generally not used but you can explore further if you wish.\n",
    "- **subsample [default=1]**\n",
    "    - Same as the subsample of GBM. Denotes the fraction of observations to be randomly samples for each tree.\n",
    "    - Lower values make the algorithm more conservative and prevents overfitting but too small values might lead to under-fitting.\n",
    "    - Typical values: 0.5-1\n",
    "- **colsample_bytree [default=1]**\n",
    "    - Similar to max_features in GBM. Denotes the fraction of columns to be randomly samples for each tree.\n",
    "    - Typical values: 0.5-1\n",
    "- **colsample_bylevel [default=1]**\n",
    "    - Denotes the subsample ratio of columns for each split, in each level.\n",
    "    - I don’t use this often because subsample and colsample_bytree will do the job for you. but you can explore further if you feel so.\n",
    "- **lambda [default=1]**\n",
    "    - L2 regularization term on weights (analogous to Ridge regression)\n",
    "    - This used to handle the regularization part of XGBoost. Though many data scientists don’t use it often, it should be explored to reduce overfitting.\n",
    "- **alpha [default=0]**\n",
    "    - L1 regularization term on weight (analogous to Lasso regression)\n",
    "    - Can be used in case of very high dimensionality so that the algorithm runs faster when implemented\n",
    "- **scale_pos_weight [default=1]**\n",
    "    - A value greater than 0 should be used in case of high class imbalance as it helps in faster convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Task Parameters\n",
    "\n",
    "These parameters are used to define the optimization objective the metric to be calculated at each step.\n",
    "\n",
    "- **objective [default=reg:linear]**\n",
    "    - This defines the loss function to be minimized. Mostly used values are:\n",
    "        - binary:logistic –logistic regression for binary classification, returns predicted probability (not class)\n",
    "        - multi:softmax –multiclass classification using the softmax objective, returns predicted class (not probabilities)\n",
    "                - you also need to set an additional num_class (number of classes) parameter defining the number of unique classes\n",
    "        - multi:softprob –same as softmax, but returns predicted probability of each data point belonging to each class.\n",
    "- **eval_metric [ default according to objective ]**\n",
    "    - The metric to be used for validation data.\n",
    "    - The default values are rmse for regression and error for classification.\n",
    "    - Typical values are:\n",
    "            - rmse – root mean square error\n",
    "            - mae – mean absolute error\n",
    "            - logloss – negative log-likelihood\n",
    "            - error – Binary classification error rate (0.5 threshold)\n",
    "            - merror – Multiclass classification error rate\n",
    "            - mlogloss – Multiclass logloss\n",
    "            - auc: Area under the curve\n",
    "- **seed [default=0]**\n",
    "    - The random number seed.\n",
    "    - Can be used for generating reproducible results and also for parameter tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Tuning with Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T19:24:58.590261Z",
     "start_time": "2019-06-25T19:24:58.575807Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "              max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "              n_jobs=1, nthread=None, objective='binary:logistic',\n",
       "              random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "              seed=None, silent=True, subsample=1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T19:24:58.600562Z",
     "start_time": "2019-06-25T19:24:58.595011Z"
    }
   },
   "outputs": [],
   "source": [
    "xg_clf = xgb.XGBClassifier(objective ='binary:logistic', \n",
    "                           colsample_bytree = 0.3, \n",
    "                           learning_rate = 0.1,\n",
    "                           max_depth = 2, \n",
    "                           alpha = 1, \n",
    "                           n_estimators = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T19:24:58.657285Z",
     "start_time": "2019-06-25T19:24:58.603801Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(alpha=1, base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bytree=0.3, gamma=0, learning_rate=0.1,\n",
       "              max_delta_step=0, max_depth=2, min_child_weight=1, missing=None,\n",
       "              n_estimators=100, n_jobs=1, nthread=None,\n",
       "              objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=1, seed=None, silent=True,\n",
       "              subsample=1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg_clf.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T19:24:58.674860Z",
     "start_time": "2019-06-25T19:24:58.661563Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.802691\n",
      "F1: 0.702703\n"
     ]
    }
   ],
   "source": [
    "preds = xg_clf.predict(X_test)\n",
    "\n",
    "\n",
    "test_f1 = f1_score(y_test, preds)\n",
    "test_acc = accuracy_score(y_test, preds)\n",
    "\n",
    "print(\"Accuracy: %f\" % (test_acc))\n",
    "print(\"F1: %f\" % (test_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-fold Cross Validation using XGBoost\n",
    "In order to build more robust models, it is common to do a k-fold cross validation where all the entries in the original training dataset are used for both training as well as validation. XGBoost supports k-fold cross validation via the cv() method. All you have to do is specify the nfolds parameter, which is the number of cross validation sets you want to build. Also, it supports many other parameters (check out this link) like:\n",
    "\n",
    "- **num_boost_round**: denotes the number of trees you build (analogous to n_estimators)\n",
    "- **metrics:** tells the evaluation metrics to be watched during CV\n",
    "- **as_pandas**: to return the results in a pandas DataFrame.\n",
    "- **early_stopping_rounds: finishes training of the model early if the hold-out metric (\"rmse\" in our case) does not improve for a given number of rounds.\n",
    "- **seed**: for reproducibility of results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running your model, you will convert the dataset into an optimized data structure called Dmatrix that XGBoost supports and gives it acclaimed performance and efficiency gains. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T19:24:58.688607Z",
     "start_time": "2019-06-25T19:24:58.678934Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/utaveras/anaconda3/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "/Users/utaveras/anaconda3/lib/python3.7/site-packages/xgboost/core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  data.base is not None and isinstance(data, np.ndarray) \\\n"
     ]
    }
   ],
   "source": [
    "data_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T19:24:59.103087Z",
     "start_time": "2019-06-25T19:24:58.693729Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params = {\"objective\":\"binary:logistic\",\n",
    "          'colsample_bytree': 0.3,\n",
    "          'learning_rate': 0.1,\n",
    "          'max_depth': 2, \n",
    "          'alpha': 1}\n",
    "\n",
    "cv_results = xgb.cv(dtrain=data_dmatrix, \n",
    "                    params=params, \n",
    "                    nfold=5,\n",
    "                    num_boost_round=500,\n",
    "                    early_stopping_rounds=5,\n",
    "                    metrics=\"logloss\", \n",
    "                    as_pandas=True, \n",
    "                    seed=123)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T19:24:59.146883Z",
     "start_time": "2019-06-25T19:24:59.106779Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-logloss-mean</th>\n",
       "      <th>train-logloss-std</th>\n",
       "      <th>test-logloss-mean</th>\n",
       "      <th>test-logloss-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.685093</td>\n",
       "      <td>0.000379</td>\n",
       "      <td>0.686558</td>\n",
       "      <td>0.000999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.671230</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>0.675102</td>\n",
       "      <td>0.008816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.659008</td>\n",
       "      <td>0.010491</td>\n",
       "      <td>0.663125</td>\n",
       "      <td>0.007590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.644336</td>\n",
       "      <td>0.014084</td>\n",
       "      <td>0.650664</td>\n",
       "      <td>0.011789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.636066</td>\n",
       "      <td>0.016528</td>\n",
       "      <td>0.643942</td>\n",
       "      <td>0.013259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.623169</td>\n",
       "      <td>0.012498</td>\n",
       "      <td>0.631285</td>\n",
       "      <td>0.012165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.602033</td>\n",
       "      <td>0.009843</td>\n",
       "      <td>0.610209</td>\n",
       "      <td>0.013601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.591650</td>\n",
       "      <td>0.015947</td>\n",
       "      <td>0.599721</td>\n",
       "      <td>0.021180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.585461</td>\n",
       "      <td>0.019497</td>\n",
       "      <td>0.593338</td>\n",
       "      <td>0.024632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.578619</td>\n",
       "      <td>0.018259</td>\n",
       "      <td>0.586940</td>\n",
       "      <td>0.022570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.570801</td>\n",
       "      <td>0.018858</td>\n",
       "      <td>0.580262</td>\n",
       "      <td>0.021636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.560062</td>\n",
       "      <td>0.022019</td>\n",
       "      <td>0.569363</td>\n",
       "      <td>0.024691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.556937</td>\n",
       "      <td>0.020256</td>\n",
       "      <td>0.566690</td>\n",
       "      <td>0.023334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.554872</td>\n",
       "      <td>0.019017</td>\n",
       "      <td>0.564808</td>\n",
       "      <td>0.022910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.546951</td>\n",
       "      <td>0.015151</td>\n",
       "      <td>0.557979</td>\n",
       "      <td>0.020297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.541852</td>\n",
       "      <td>0.011557</td>\n",
       "      <td>0.553449</td>\n",
       "      <td>0.020814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.536363</td>\n",
       "      <td>0.015298</td>\n",
       "      <td>0.548007</td>\n",
       "      <td>0.021016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.529293</td>\n",
       "      <td>0.017539</td>\n",
       "      <td>0.541409</td>\n",
       "      <td>0.021934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.523798</td>\n",
       "      <td>0.019147</td>\n",
       "      <td>0.536885</td>\n",
       "      <td>0.024915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.520973</td>\n",
       "      <td>0.019159</td>\n",
       "      <td>0.534507</td>\n",
       "      <td>0.025665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.516376</td>\n",
       "      <td>0.021708</td>\n",
       "      <td>0.530500</td>\n",
       "      <td>0.025853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.513121</td>\n",
       "      <td>0.022248</td>\n",
       "      <td>0.527367</td>\n",
       "      <td>0.026611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.509301</td>\n",
       "      <td>0.024330</td>\n",
       "      <td>0.523904</td>\n",
       "      <td>0.027239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.502340</td>\n",
       "      <td>0.024646</td>\n",
       "      <td>0.517755</td>\n",
       "      <td>0.029960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.495447</td>\n",
       "      <td>0.019774</td>\n",
       "      <td>0.510592</td>\n",
       "      <td>0.026663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.489835</td>\n",
       "      <td>0.016329</td>\n",
       "      <td>0.505060</td>\n",
       "      <td>0.024653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.488421</td>\n",
       "      <td>0.016179</td>\n",
       "      <td>0.504394</td>\n",
       "      <td>0.024912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.485823</td>\n",
       "      <td>0.015821</td>\n",
       "      <td>0.501844</td>\n",
       "      <td>0.026256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.480789</td>\n",
       "      <td>0.013510</td>\n",
       "      <td>0.498244</td>\n",
       "      <td>0.023527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.479223</td>\n",
       "      <td>0.012897</td>\n",
       "      <td>0.496588</td>\n",
       "      <td>0.025181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.401454</td>\n",
       "      <td>0.010871</td>\n",
       "      <td>0.435937</td>\n",
       "      <td>0.035241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0.401162</td>\n",
       "      <td>0.010739</td>\n",
       "      <td>0.435649</td>\n",
       "      <td>0.035531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0.400962</td>\n",
       "      <td>0.010795</td>\n",
       "      <td>0.435636</td>\n",
       "      <td>0.035789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0.400442</td>\n",
       "      <td>0.010828</td>\n",
       "      <td>0.435524</td>\n",
       "      <td>0.036278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0.400060</td>\n",
       "      <td>0.010824</td>\n",
       "      <td>0.435140</td>\n",
       "      <td>0.036231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0.399748</td>\n",
       "      <td>0.010698</td>\n",
       "      <td>0.435059</td>\n",
       "      <td>0.036187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0.399014</td>\n",
       "      <td>0.010926</td>\n",
       "      <td>0.434782</td>\n",
       "      <td>0.036432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>0.398543</td>\n",
       "      <td>0.010926</td>\n",
       "      <td>0.434627</td>\n",
       "      <td>0.036380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>0.398242</td>\n",
       "      <td>0.010878</td>\n",
       "      <td>0.434306</td>\n",
       "      <td>0.036436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0.398027</td>\n",
       "      <td>0.010912</td>\n",
       "      <td>0.434199</td>\n",
       "      <td>0.036489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>0.397627</td>\n",
       "      <td>0.011141</td>\n",
       "      <td>0.433942</td>\n",
       "      <td>0.036524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>0.397133</td>\n",
       "      <td>0.011175</td>\n",
       "      <td>0.433835</td>\n",
       "      <td>0.037015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0.396799</td>\n",
       "      <td>0.011203</td>\n",
       "      <td>0.433926</td>\n",
       "      <td>0.037013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>0.396528</td>\n",
       "      <td>0.011347</td>\n",
       "      <td>0.433716</td>\n",
       "      <td>0.037159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>0.396403</td>\n",
       "      <td>0.011355</td>\n",
       "      <td>0.433744</td>\n",
       "      <td>0.037349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>0.396163</td>\n",
       "      <td>0.011408</td>\n",
       "      <td>0.433820</td>\n",
       "      <td>0.037432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>0.395454</td>\n",
       "      <td>0.011635</td>\n",
       "      <td>0.433707</td>\n",
       "      <td>0.037657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0.395105</td>\n",
       "      <td>0.011678</td>\n",
       "      <td>0.433470</td>\n",
       "      <td>0.037710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>0.394756</td>\n",
       "      <td>0.011652</td>\n",
       "      <td>0.433355</td>\n",
       "      <td>0.037991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>0.394392</td>\n",
       "      <td>0.011326</td>\n",
       "      <td>0.432757</td>\n",
       "      <td>0.038672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>0.394160</td>\n",
       "      <td>0.011433</td>\n",
       "      <td>0.432578</td>\n",
       "      <td>0.038866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0.393845</td>\n",
       "      <td>0.011346</td>\n",
       "      <td>0.432501</td>\n",
       "      <td>0.039088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0.393636</td>\n",
       "      <td>0.011348</td>\n",
       "      <td>0.432425</td>\n",
       "      <td>0.039194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>0.393192</td>\n",
       "      <td>0.011366</td>\n",
       "      <td>0.432096</td>\n",
       "      <td>0.039302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0.392460</td>\n",
       "      <td>0.011382</td>\n",
       "      <td>0.431824</td>\n",
       "      <td>0.039392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>0.392240</td>\n",
       "      <td>0.011336</td>\n",
       "      <td>0.431829</td>\n",
       "      <td>0.039511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0.391888</td>\n",
       "      <td>0.011091</td>\n",
       "      <td>0.431388</td>\n",
       "      <td>0.040072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>0.391675</td>\n",
       "      <td>0.011100</td>\n",
       "      <td>0.431431</td>\n",
       "      <td>0.040272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>0.391332</td>\n",
       "      <td>0.011102</td>\n",
       "      <td>0.431478</td>\n",
       "      <td>0.040197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0.391117</td>\n",
       "      <td>0.011108</td>\n",
       "      <td>0.431293</td>\n",
       "      <td>0.040253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>132 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     train-logloss-mean  train-logloss-std  test-logloss-mean  \\\n",
       "0              0.685093           0.000379           0.686558   \n",
       "1              0.671230           0.009434           0.675102   \n",
       "2              0.659008           0.010491           0.663125   \n",
       "3              0.644336           0.014084           0.650664   \n",
       "4              0.636066           0.016528           0.643942   \n",
       "5              0.623169           0.012498           0.631285   \n",
       "6              0.602033           0.009843           0.610209   \n",
       "7              0.591650           0.015947           0.599721   \n",
       "8              0.585461           0.019497           0.593338   \n",
       "9              0.578619           0.018259           0.586940   \n",
       "10             0.570801           0.018858           0.580262   \n",
       "11             0.560062           0.022019           0.569363   \n",
       "12             0.556937           0.020256           0.566690   \n",
       "13             0.554872           0.019017           0.564808   \n",
       "14             0.546951           0.015151           0.557979   \n",
       "15             0.541852           0.011557           0.553449   \n",
       "16             0.536363           0.015298           0.548007   \n",
       "17             0.529293           0.017539           0.541409   \n",
       "18             0.523798           0.019147           0.536885   \n",
       "19             0.520973           0.019159           0.534507   \n",
       "20             0.516376           0.021708           0.530500   \n",
       "21             0.513121           0.022248           0.527367   \n",
       "22             0.509301           0.024330           0.523904   \n",
       "23             0.502340           0.024646           0.517755   \n",
       "24             0.495447           0.019774           0.510592   \n",
       "25             0.489835           0.016329           0.505060   \n",
       "26             0.488421           0.016179           0.504394   \n",
       "27             0.485823           0.015821           0.501844   \n",
       "28             0.480789           0.013510           0.498244   \n",
       "29             0.479223           0.012897           0.496588   \n",
       "..                  ...                ...                ...   \n",
       "102            0.401454           0.010871           0.435937   \n",
       "103            0.401162           0.010739           0.435649   \n",
       "104            0.400962           0.010795           0.435636   \n",
       "105            0.400442           0.010828           0.435524   \n",
       "106            0.400060           0.010824           0.435140   \n",
       "107            0.399748           0.010698           0.435059   \n",
       "108            0.399014           0.010926           0.434782   \n",
       "109            0.398543           0.010926           0.434627   \n",
       "110            0.398242           0.010878           0.434306   \n",
       "111            0.398027           0.010912           0.434199   \n",
       "112            0.397627           0.011141           0.433942   \n",
       "113            0.397133           0.011175           0.433835   \n",
       "114            0.396799           0.011203           0.433926   \n",
       "115            0.396528           0.011347           0.433716   \n",
       "116            0.396403           0.011355           0.433744   \n",
       "117            0.396163           0.011408           0.433820   \n",
       "118            0.395454           0.011635           0.433707   \n",
       "119            0.395105           0.011678           0.433470   \n",
       "120            0.394756           0.011652           0.433355   \n",
       "121            0.394392           0.011326           0.432757   \n",
       "122            0.394160           0.011433           0.432578   \n",
       "123            0.393845           0.011346           0.432501   \n",
       "124            0.393636           0.011348           0.432425   \n",
       "125            0.393192           0.011366           0.432096   \n",
       "126            0.392460           0.011382           0.431824   \n",
       "127            0.392240           0.011336           0.431829   \n",
       "128            0.391888           0.011091           0.431388   \n",
       "129            0.391675           0.011100           0.431431   \n",
       "130            0.391332           0.011102           0.431478   \n",
       "131            0.391117           0.011108           0.431293   \n",
       "\n",
       "     test-logloss-std  \n",
       "0            0.000999  \n",
       "1            0.008816  \n",
       "2            0.007590  \n",
       "3            0.011789  \n",
       "4            0.013259  \n",
       "5            0.012165  \n",
       "6            0.013601  \n",
       "7            0.021180  \n",
       "8            0.024632  \n",
       "9            0.022570  \n",
       "10           0.021636  \n",
       "11           0.024691  \n",
       "12           0.023334  \n",
       "13           0.022910  \n",
       "14           0.020297  \n",
       "15           0.020814  \n",
       "16           0.021016  \n",
       "17           0.021934  \n",
       "18           0.024915  \n",
       "19           0.025665  \n",
       "20           0.025853  \n",
       "21           0.026611  \n",
       "22           0.027239  \n",
       "23           0.029960  \n",
       "24           0.026663  \n",
       "25           0.024653  \n",
       "26           0.024912  \n",
       "27           0.026256  \n",
       "28           0.023527  \n",
       "29           0.025181  \n",
       "..                ...  \n",
       "102          0.035241  \n",
       "103          0.035531  \n",
       "104          0.035789  \n",
       "105          0.036278  \n",
       "106          0.036231  \n",
       "107          0.036187  \n",
       "108          0.036432  \n",
       "109          0.036380  \n",
       "110          0.036436  \n",
       "111          0.036489  \n",
       "112          0.036524  \n",
       "113          0.037015  \n",
       "114          0.037013  \n",
       "115          0.037159  \n",
       "116          0.037349  \n",
       "117          0.037432  \n",
       "118          0.037657  \n",
       "119          0.037710  \n",
       "120          0.037991  \n",
       "121          0.038672  \n",
       "122          0.038866  \n",
       "123          0.039088  \n",
       "124          0.039194  \n",
       "125          0.039302  \n",
       "126          0.039392  \n",
       "127          0.039511  \n",
       "128          0.040072  \n",
       "129          0.040272  \n",
       "130          0.040197  \n",
       "131          0.040253  \n",
       "\n",
       "[132 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T19:48:32.553349Z",
     "start_time": "2019-06-25T19:48:32.543247Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "%matplotlib inline\n",
    "style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T19:48:48.194941Z",
     "start_time": "2019-06-25T19:48:47.966669Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xgb.plot_importance(xg_clf)\n",
    "plt.rcParams['figure.figsize'] = [5, 5]\n",
    "style.use('seaborn-ticks')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T19:24:59.774095Z",
     "start_time": "2019-06-25T19:24:59.756783Z"
    }
   },
   "outputs": [],
   "source": [
    "def modelfit(alg, dtrain, predictors, target, useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(dtrain[predictors].values, label=dtrain[target].values)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "            metrics='auc', early_stopping_rounds=early_stopping_rounds)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(dtrain[predictors], dtrain[target],eval_metric='auc')\n",
    "        \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(dtrain[predictors])\n",
    "    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n",
    "        \n",
    "    #Print model report:\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"Accuracy : %.4g\" % accuracy_score(dtrain[target].values, dtrain_predictions))\n",
    "    print(\"AUC Score (Train): %f\" % roc_auc_score(dtrain[target], dtrain_predprob))\n",
    "\n",
    "    return alg\n",
    "#     feat_imp = pd.Series(alg.get_booster().get_fscore())\n",
    "#     feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "#     plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T19:24:59.791719Z",
     "start_time": "2019-06-25T19:24:59.777664Z"
    }
   },
   "outputs": [],
   "source": [
    "train  = pd.concat([X_train, y_train], axis=1)\n",
    "target = 'Survived'\n",
    "IDcol = 'PassengerId'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T19:24:59.816290Z",
     "start_time": "2019-06-25T19:24:59.794473Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>youngin</th>\n",
       "      <th>male</th>\n",
       "      <th>Q</th>\n",
       "      <th>S</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>3</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>3</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34.3750</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>3</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>3</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8542</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Pclass   Age  SibSp  Parch     Fare  youngin  male  Q  S  \\\n",
       "PassengerId                                                             \n",
       "740               3  24.0      0      0   7.8958        0     1  0  1   \n",
       "148               3   9.0      2      2  34.3750        1     0  0  1   \n",
       "876               3  15.0      0      0   7.2250        0     0  0  0   \n",
       "641               3  20.0      0      0   7.8542        0     1  0  1   \n",
       "885               3  25.0      0      0   7.0500        0     1  0  1   \n",
       "\n",
       "             Survived  \n",
       "PassengerId            \n",
       "740                 0  \n",
       "148                 0  \n",
       "876                 1  \n",
       "641                 0  \n",
       "885                 0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T19:25:00.554047Z",
     "start_time": "2019-06-25T19:24:59.820179Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "Accuracy : 0.8829\n",
      "AUC Score (Train): 0.928483\n"
     ]
    }
   ],
   "source": [
    "#Choose all predictors except target & IDcols\n",
    "predictors = [x for x in train.columns if x not in [target, IDcol]]\n",
    "xgb1 = xgb.XGBClassifier(\n",
    " learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=3,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.6,\n",
    " colsample_bytree=0.3,\n",
    " objective= 'binary:logistic',\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=27)\n",
    "\n",
    "alg = modelfit(xgb1, train, predictors, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T19:25:00.572783Z",
     "start_time": "2019-06-25T19:25:00.558693Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.807175\n",
      "F1: 0.703448\n"
     ]
    }
   ],
   "source": [
    "preds = alg.predict(X_test)\n",
    "\n",
    "\n",
    "test_f1 = f1_score(y_test, preds)\n",
    "test_acc = accuracy_score(y_test, preds)\n",
    "\n",
    "print(\"Accuracy: %f\" % (test_acc))\n",
    "print(\"F1: %f\" % (test_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining XGBoost with GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T20:03:42.128448Z",
     "start_time": "2019-06-25T20:03:42.121680Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T20:03:42.147648Z",
     "start_time": "2019-06-25T20:03:42.142237Z"
    }
   },
   "outputs": [],
   "source": [
    "#setup our parameters for XGBoost to test\n",
    "param_test1 = {\n",
    "    'max_depth': range(2, 9, 2),\n",
    "    'min_child_weight': range(1, 6, 2),\n",
    "    'colsample_bytree': [0.25, 0.5, 0.75, 1],\n",
    "    'learning_rate':[0.1, 0.5, 0.75, 1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T20:03:42.160607Z",
     "start_time": "2019-06-25T20:03:42.155227Z"
    }
   },
   "outputs": [],
   "source": [
    "# initiate the Gridsearch model\n",
    "gsearch1 = GridSearchCV(\n",
    "    estimator=xgb.XGBClassifier(\n",
    "        n_estimators=140,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        objective='binary:logistic',\n",
    "        nthread=4,\n",
    "        scale_pos_weight=1,\n",
    "        seed=27),\n",
    "    param_grid=param_test1,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    iid=False,\n",
    "    cv=5,\n",
    "    verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T20:05:07.785107Z",
     "start_time": "2019-06-25T20:03:42.164285Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 192 candidates, totalling 960 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    5.5s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   14.4s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:   31.0s\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:   54.4s\n",
      "[Parallel(n_jobs=-1)]: Done 960 out of 960 | elapsed:  1.4min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "                                     colsample_bylevel=1, colsample_bytree=1,\n",
       "                                     gamma=0, learning_rate=0.1,\n",
       "                                     max_delta_step=0, max_depth=3,\n",
       "                                     min_child_weight=1, missing=None,\n",
       "                                     n_estimators=140, n_jobs=1, nthread=4,\n",
       "                                     objective='binary:logistic',\n",
       "                                     random_state=0, reg_alpha=0, reg_lambda=1,\n",
       "                                     scale_pos_weight=1, seed=27, silent=True,\n",
       "                                     subsample=0.8),\n",
       "             iid=False, n_jobs=-1,\n",
       "             param_grid={'colsample_bytree': [0.25, 0.5, 0.75, 1],\n",
       "                         'learning_rate': [0.1, 0.5, 0.75, 1],\n",
       "                         'max_depth': range(2, 9, 2),\n",
       "                         'min_child_weight': range(1, 6, 2)},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='f1', verbose=2)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.fit(train[predictors],train[target])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T20:05:07.997522Z",
     "start_time": "2019-06-25T20:05:07.792702Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.49651318, 0.36619024, 0.68214927, 0.29451685, 0.74787469,\n",
       "        0.41753149, 0.58226886, 0.6652472 , 0.9921886 , 0.75122948,\n",
       "        0.56691484, 0.44259305, 0.32657981, 0.40342135, 0.78108144,\n",
       "        0.46262875, 0.51639576, 0.56081371, 0.35106997, 0.89667578,\n",
       "        0.82985163, 0.88797078, 0.66622882, 0.26637149, 0.29865193,\n",
       "        0.60628462, 0.5754365 , 0.49739799, 0.67642407, 0.33950434,\n",
       "        0.69287343, 0.76548805, 0.62318363, 0.73056073, 0.49411659,\n",
       "        0.67370701, 0.17178125, 0.59319944, 0.50767241, 0.64438162,\n",
       "        0.37864304, 0.37937961, 0.56962013, 0.73113403, 0.88002124,\n",
       "        0.56994619, 0.52082539, 0.76117516, 0.31871138, 0.60966253,\n",
       "        0.3923243 , 0.51447349, 0.66927257, 0.64513292, 1.10198498,\n",
       "        0.68994379, 0.80443802, 0.89430165, 1.47031336, 0.36728678,\n",
       "        0.72036939, 0.4583344 , 0.40339837, 0.44456096, 0.52120628,\n",
       "        0.83863111, 0.94748001, 0.41743798, 0.80258765, 0.92584367,\n",
       "        0.51513638, 0.88765707, 0.38671336, 0.32551079, 0.4641232 ,\n",
       "        0.89921994, 0.5918829 , 0.69007244, 0.61270194, 0.41370645,\n",
       "        0.7011867 , 1.15374627, 0.25215149, 0.93705149, 0.36164608,\n",
       "        0.39797778, 0.33557353, 0.77764001, 0.65466938, 0.37842312,\n",
       "        0.55947351, 0.96511292, 0.49100537, 1.03566279, 0.32458272,\n",
       "        0.98534975, 0.27394319, 0.55610118, 0.33766699, 0.53330646,\n",
       "        0.717413  , 0.84798632, 0.81657767, 0.92378564, 0.59674048,\n",
       "        1.28014393, 0.79857445, 0.81438036, 0.22129059, 0.55193601,\n",
       "        0.53285365, 0.59774899, 0.53839526, 0.63694   , 0.94936795,\n",
       "        0.80186181, 0.58703513, 1.4256988 , 0.92567501, 0.72462983,\n",
       "        0.32111101, 0.56167879, 0.44303584, 0.62501998, 0.60069528,\n",
       "        0.74373055, 1.02912679, 0.53158932, 0.6702991 , 0.87596469,\n",
       "        0.55266194, 0.89919987, 0.58097596, 0.41180434, 0.45797977,\n",
       "        0.59581046, 0.57603536, 0.99415798, 0.56423211, 0.88286667,\n",
       "        0.39091091, 1.00415087, 0.92339025, 0.89426403, 0.27125998,\n",
       "        0.3149756 , 0.25436726, 0.95090122, 0.99671664, 0.65062628,\n",
       "        1.06158643, 0.80941639, 1.09462175, 1.34799218, 0.91156158,\n",
       "        1.12955661, 0.43563027, 0.40096703, 0.45182662, 1.06876664,\n",
       "        0.89485335, 0.57514687, 0.77160206, 1.09813151, 1.27681851,\n",
       "        0.92827992, 0.74046597, 0.65934663, 0.51155481, 0.68359199,\n",
       "        0.46625781, 0.7293232 , 0.54659414, 0.84570127, 0.85257239,\n",
       "        0.93201237, 0.48799357, 1.2104178 , 0.62301688, 1.11582675,\n",
       "        0.33862057, 0.37328715, 0.50758419, 0.67694144, 0.81718321,\n",
       "        0.77880311, 0.68875694, 0.89414234, 0.85182862, 0.94819617,\n",
       "        0.52079935, 0.74556861]),\n",
       " 'std_fit_time': array([0.05066471, 0.12240216, 0.01432139, 0.3015252 , 0.45127175,\n",
       "        0.35784187, 0.34125884, 0.18007227, 0.3273357 , 0.53927478,\n",
       "        0.44683304, 0.29307922, 0.0324493 , 0.07667659, 0.27108768,\n",
       "        0.37134953, 0.48748918, 0.30218794, 0.17363916, 0.34744349,\n",
       "        0.51563454, 0.53884688, 0.60255093, 0.26060611, 0.20231468,\n",
       "        0.15967589, 0.39038498, 0.48308006, 0.4148283 , 0.31943307,\n",
       "        0.536055  , 0.50745564, 0.40329702, 0.63912375, 0.3751883 ,\n",
       "        0.47965954, 0.21857949, 0.1632536 , 0.36052047, 0.46359059,\n",
       "        0.26091598, 0.377764  , 0.25769913, 0.39058792, 0.45599121,\n",
       "        0.32388477, 0.50269806, 0.37773997, 0.26411585, 0.27462196,\n",
       "        0.2640029 , 0.46176252, 0.4536759 , 0.64478813, 0.61024714,\n",
       "        0.4100086 , 0.46016666, 0.7274297 , 0.74839775, 0.36784978,\n",
       "        0.54407091, 0.48592773, 0.42086577, 0.44972465, 0.32206527,\n",
       "        0.40618015, 0.73352688, 0.37693794, 0.44297638, 0.67623764,\n",
       "        0.45845775, 0.54576747, 0.33094731, 0.28110924, 0.21768789,\n",
       "        0.7111155 , 0.44583731, 0.5092522 , 0.442657  , 0.35642639,\n",
       "        0.54944954, 0.61480322, 0.17816738, 0.69516445, 0.2752594 ,\n",
       "        0.3959688 , 0.32413182, 0.64580028, 0.48564314, 0.29897568,\n",
       "        0.62685712, 0.50916713, 0.44367023, 0.7836553 , 0.23250344,\n",
       "        0.73472473, 0.26177717, 0.40785472, 0.38244766, 0.55382179,\n",
       "        0.48704501, 0.53513117, 0.70759992, 0.61410663, 0.58272011,\n",
       "        0.67959813, 0.77677876, 0.62270241, 0.19543583, 0.39665049,\n",
       "        0.38645367, 0.53660416, 0.40376416, 0.28402721, 0.76277872,\n",
       "        0.63182968, 0.53375566, 0.63952078, 0.70691578, 0.60632451,\n",
       "        0.31499064, 0.29673962, 0.28311521, 0.52258466, 0.37517836,\n",
       "        0.52849068, 0.64708316, 0.57964489, 0.48231066, 0.60015924,\n",
       "        0.37145347, 0.36450456, 0.45463084, 0.28399998, 0.43062262,\n",
       "        0.39255901, 0.41726874, 0.20753794, 0.53420855, 0.45094927,\n",
       "        0.21368216, 0.6165677 , 0.50508077, 0.6418296 , 0.33840211,\n",
       "        0.24993146, 0.24263679, 0.3567444 , 0.379887  , 0.49498729,\n",
       "        0.60858537, 0.63182494, 0.33137928, 0.50021661, 0.68523912,\n",
       "        0.39899814, 0.31026781, 0.36558499, 0.35441474, 0.45694531,\n",
       "        0.44340653, 0.48317948, 0.66524344, 0.61839577, 0.4399897 ,\n",
       "        0.61234166, 0.78734981, 0.40423628, 0.42220586, 0.47063587,\n",
       "        0.38989556, 0.48773564, 0.37532636, 0.27860018, 0.6189144 ,\n",
       "        0.55160716, 0.32007544, 0.52200003, 0.35288513, 0.50862975,\n",
       "        0.33213096, 0.16020139, 0.17248191, 0.27404593, 0.54916905,\n",
       "        0.3327083 , 0.43937212, 0.24890311, 0.39908286, 0.41846153,\n",
       "        0.27783451, 0.04648679]),\n",
       " 'mean_score_time': array([0.00446997, 0.00552673, 0.00561614, 0.0043952 , 0.0043509 ,\n",
       "        0.00567393, 0.00473828, 0.00567579, 0.0080195 , 0.00435138,\n",
       "        0.0064467 , 0.00467949, 0.00539656, 0.00591111, 0.00795918,\n",
       "        0.00624881, 0.00561137, 0.00565476, 0.00697546, 0.00903587,\n",
       "        0.00500393, 0.00659757, 0.00748   , 0.00990524, 0.0054893 ,\n",
       "        0.00660281, 0.00543141, 0.00749135, 0.00493751, 0.00708666,\n",
       "        0.00644617, 0.005685  , 0.00471649, 0.00762925, 0.00518718,\n",
       "        0.00699182, 0.00528169, 0.0059936 , 0.00635438, 0.00682759,\n",
       "        0.00478611, 0.00509386, 0.00575271, 0.00552344, 0.00578923,\n",
       "        0.00582752, 0.00508118, 0.00636678, 0.00842676, 0.00547123,\n",
       "        0.00659933, 0.00459561, 0.00616255, 0.00804753, 0.00705519,\n",
       "        0.00481896, 0.00597863, 0.01579881, 0.00536146, 0.00535674,\n",
       "        0.00502086, 0.00631127, 0.00518451, 0.00565505, 0.00716453,\n",
       "        0.00578918, 0.00537257, 0.00751033, 0.0054512 , 0.00628037,\n",
       "        0.0077662 , 0.00554156, 0.0064918 , 0.00562415, 0.00802817,\n",
       "        0.00562849, 0.00515103, 0.00540557, 0.00594845, 0.00564756,\n",
       "        0.00548091, 0.00566983, 0.00616059, 0.0085309 , 0.00565934,\n",
       "        0.00552545, 0.00701723, 0.00650291, 0.00605736, 0.00525336,\n",
       "        0.00707226, 0.00479145, 0.00588498, 0.00589662, 0.00856009,\n",
       "        0.00664377, 0.00492902, 0.00558066, 0.0072824 , 0.00540123,\n",
       "        0.00526876, 0.00577779, 0.0055593 , 0.00549593, 0.00642152,\n",
       "        0.0049562 , 0.00518098, 0.00636783, 0.00685911, 0.00699434,\n",
       "        0.00813489, 0.00699449, 0.00651317, 0.00583296, 0.00511551,\n",
       "        0.00575433, 0.00663409, 0.00687065, 0.00583367, 0.00649395,\n",
       "        0.00646057, 0.00614486, 0.00536823, 0.00572219, 0.00565801,\n",
       "        0.00915055, 0.00564981, 0.00539474, 0.00554733, 0.00594316,\n",
       "        0.00549889, 0.00639544, 0.00629721, 0.00522318, 0.00589662,\n",
       "        0.00614676, 0.00583558, 0.00591655, 0.00573516, 0.00534372,\n",
       "        0.00500145, 0.00569963, 0.00646877, 0.00626197, 0.00705643,\n",
       "        0.00567956, 0.00498319, 0.00568776, 0.00515599, 0.00562916,\n",
       "        0.00549684, 0.00565906, 0.00647898, 0.01017098, 0.00461302,\n",
       "        0.01189489, 0.00604358, 0.00459838, 0.00572195, 0.00543103,\n",
       "        0.00884166, 0.00588145, 0.00544481, 0.0052815 , 0.00629077,\n",
       "        0.00560255, 0.00622559, 0.0049964 , 0.00563436, 0.00491676,\n",
       "        0.00598474, 0.00536776, 0.00498562, 0.00509591, 0.01018   ,\n",
       "        0.00796604, 0.0060318 , 0.0064024 , 0.01461697, 0.00523748,\n",
       "        0.00577259, 0.00519347, 0.00539217, 0.00650563, 0.00523171,\n",
       "        0.00521841, 0.0079463 , 0.00605059, 0.00442939, 0.00884919,\n",
       "        0.00555   , 0.00554285]),\n",
       " 'std_score_time': array([0.00016527, 0.00094938, 0.00092207, 0.00054588, 0.00072525,\n",
       "        0.00196774, 0.00055398, 0.00203661, 0.00438643, 0.00092298,\n",
       "        0.0015668 , 0.00043315, 0.00148009, 0.00116669, 0.00412295,\n",
       "        0.00167221, 0.00146917, 0.00106672, 0.00406867, 0.0070336 ,\n",
       "        0.00078271, 0.00251946, 0.00385698, 0.00527619, 0.00191418,\n",
       "        0.00229482, 0.00085195, 0.00224775, 0.00168617, 0.00350086,\n",
       "        0.00174578, 0.00110423, 0.00136299, 0.00343889, 0.00128554,\n",
       "        0.0016737 , 0.00090607, 0.00140179, 0.00119256, 0.00192644,\n",
       "        0.0011093 , 0.00152272, 0.00047768, 0.00083989, 0.00101787,\n",
       "        0.00056954, 0.00112101, 0.00147871, 0.00372531, 0.00125561,\n",
       "        0.00159172, 0.00014579, 0.00177008, 0.00183147, 0.0023246 ,\n",
       "        0.00056626, 0.00087379, 0.01315226, 0.0008662 , 0.00053481,\n",
       "        0.00069383, 0.00278899, 0.00074206, 0.00068028, 0.00312975,\n",
       "        0.00087371, 0.00129096, 0.00207101, 0.00073033, 0.00196813,\n",
       "        0.00270906, 0.00101754, 0.0023182 , 0.00216796, 0.00316513,\n",
       "        0.00073509, 0.00135954, 0.00075687, 0.00090566, 0.00065671,\n",
       "        0.00120134, 0.00160052, 0.00049194, 0.00506863, 0.00150103,\n",
       "        0.00187324, 0.00217133, 0.00076323, 0.00077089, 0.00060599,\n",
       "        0.00300811, 0.00033991, 0.00086073, 0.0012314 , 0.00374492,\n",
       "        0.00110156, 0.00063259, 0.00078597, 0.00246784, 0.00037699,\n",
       "        0.00113514, 0.0010804 , 0.00131489, 0.00037093, 0.00152894,\n",
       "        0.00061205, 0.00049217, 0.00131264, 0.00279569, 0.00274055,\n",
       "        0.00171927, 0.00151438, 0.00230082, 0.0014976 , 0.00086228,\n",
       "        0.00123877, 0.00247092, 0.00164506, 0.00151787, 0.00172306,\n",
       "        0.00264264, 0.00147319, 0.00140235, 0.00069769, 0.00156203,\n",
       "        0.00859725, 0.00054845, 0.001508  , 0.00243983, 0.00146206,\n",
       "        0.00102091, 0.00191491, 0.00223414, 0.00081359, 0.00145429,\n",
       "        0.0010378 , 0.00075181, 0.00087897, 0.00070984, 0.00063184,\n",
       "        0.00094239, 0.0010176 , 0.00270529, 0.00141623, 0.00224996,\n",
       "        0.0007393 , 0.00119272, 0.00172031, 0.00077536, 0.00101956,\n",
       "        0.00102989, 0.00202743, 0.00217058, 0.00933966, 0.00021778,\n",
       "        0.01338389, 0.00098479, 0.00108994, 0.00149756, 0.00118334,\n",
       "        0.0070492 , 0.00320725, 0.00032131, 0.00130837, 0.00187938,\n",
       "        0.00097665, 0.00264208, 0.00066137, 0.00196431, 0.00176957,\n",
       "        0.00166116, 0.00061512, 0.00047301, 0.00106988, 0.00753631,\n",
       "        0.00498004, 0.00102886, 0.00348773, 0.01215127, 0.00103429,\n",
       "        0.00115626, 0.001163  , 0.00050696, 0.00359223, 0.00150479,\n",
       "        0.0010625 , 0.00227059, 0.00164501, 0.0004021 , 0.00788945,\n",
       "        0.00203113, 0.00099714]),\n",
       " 'param_colsample_bytree': masked_array(data=[0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,\n",
       "                    0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,\n",
       "                    0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,\n",
       "                    0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,\n",
       "                    0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,\n",
       "                    0.25, 0.25, 0.25, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.75, 0.75,\n",
       "                    0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75,\n",
       "                    0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75,\n",
       "                    0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75,\n",
       "                    0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75,\n",
       "                    0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75,\n",
       "                    0.75, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_learning_rate': masked_array(data=[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75,\n",
       "                    0.75, 0.75, 0.75, 0.75, 0.75, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "                    1, 1, 1, 1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.75, 0.75, 0.75, 0.75, 0.75,\n",
       "                    0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 1, 1, 1, 1,\n",
       "                    1, 1, 1, 1, 1, 1, 1, 1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.75, 0.75, 0.75,\n",
       "                    0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75,\n",
       "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.75,\n",
       "                    0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75,\n",
       "                    0.75, 0.75, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_max_depth': masked_array(data=[2, 2, 2, 4, 4, 4, 6, 6, 6, 8, 8, 8, 2, 2, 2, 4, 4, 4,\n",
       "                    6, 6, 6, 8, 8, 8, 2, 2, 2, 4, 4, 4, 6, 6, 6, 8, 8, 8,\n",
       "                    2, 2, 2, 4, 4, 4, 6, 6, 6, 8, 8, 8, 2, 2, 2, 4, 4, 4,\n",
       "                    6, 6, 6, 8, 8, 8, 2, 2, 2, 4, 4, 4, 6, 6, 6, 8, 8, 8,\n",
       "                    2, 2, 2, 4, 4, 4, 6, 6, 6, 8, 8, 8, 2, 2, 2, 4, 4, 4,\n",
       "                    6, 6, 6, 8, 8, 8, 2, 2, 2, 4, 4, 4, 6, 6, 6, 8, 8, 8,\n",
       "                    2, 2, 2, 4, 4, 4, 6, 6, 6, 8, 8, 8, 2, 2, 2, 4, 4, 4,\n",
       "                    6, 6, 6, 8, 8, 8, 2, 2, 2, 4, 4, 4, 6, 6, 6, 8, 8, 8,\n",
       "                    2, 2, 2, 4, 4, 4, 6, 6, 6, 8, 8, 8, 2, 2, 2, 4, 4, 4,\n",
       "                    6, 6, 6, 8, 8, 8, 2, 2, 2, 4, 4, 4, 6, 6, 6, 8, 8, 8,\n",
       "                    2, 2, 2, 4, 4, 4, 6, 6, 6, 8, 8, 8],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_min_child_weight': masked_array(data=[1, 3, 5, 1, 3, 5, 1, 3, 5, 1, 3, 5, 1, 3, 5, 1, 3, 5,\n",
       "                    1, 3, 5, 1, 3, 5, 1, 3, 5, 1, 3, 5, 1, 3, 5, 1, 3, 5,\n",
       "                    1, 3, 5, 1, 3, 5, 1, 3, 5, 1, 3, 5, 1, 3, 5, 1, 3, 5,\n",
       "                    1, 3, 5, 1, 3, 5, 1, 3, 5, 1, 3, 5, 1, 3, 5, 1, 3, 5,\n",
       "                    1, 3, 5, 1, 3, 5, 1, 3, 5, 1, 3, 5, 1, 3, 5, 1, 3, 5,\n",
       "                    1, 3, 5, 1, 3, 5, 1, 3, 5, 1, 3, 5, 1, 3, 5, 1, 3, 5,\n",
       "                    1, 3, 5, 1, 3, 5, 1, 3, 5, 1, 3, 5, 1, 3, 5, 1, 3, 5,\n",
       "                    1, 3, 5, 1, 3, 5, 1, 3, 5, 1, 3, 5, 1, 3, 5, 1, 3, 5,\n",
       "                    1, 3, 5, 1, 3, 5, 1, 3, 5, 1, 3, 5, 1, 3, 5, 1, 3, 5,\n",
       "                    1, 3, 5, 1, 3, 5, 1, 3, 5, 1, 3, 5, 1, 3, 5, 1, 3, 5,\n",
       "                    1, 3, 5, 1, 3, 5, 1, 3, 5, 1, 3, 5],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.75,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.75,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.75,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.75,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.75,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.75,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.75,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.75,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.75,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.75,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.75,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.75,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.75,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.75,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.75,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.75,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.75,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.75,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.75,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.75,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.75,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.75,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.75,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.75,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.75,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.75,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.75,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.75,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.75,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.75,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.75,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.75,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.75,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.75,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.75,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.75,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.75,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.75,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.75,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.75,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.75,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.75,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.75,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.75,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.75,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.75,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.75,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.75,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.75,\n",
       "   'learning_rate': 0.75,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.75,\n",
       "   'learning_rate': 0.75,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.75,\n",
       "   'learning_rate': 0.75,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.75,\n",
       "   'learning_rate': 0.75,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.75,\n",
       "   'learning_rate': 0.75,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.75,\n",
       "   'learning_rate': 0.75,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.75,\n",
       "   'learning_rate': 0.75,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.75,\n",
       "   'learning_rate': 0.75,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.75,\n",
       "   'learning_rate': 0.75,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.75,\n",
       "   'learning_rate': 0.75,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.75,\n",
       "   'learning_rate': 0.75,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.75,\n",
       "   'learning_rate': 0.75,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.75,\n",
       "   'learning_rate': 1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.75,\n",
       "   'learning_rate': 1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.75,\n",
       "   'learning_rate': 1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.75,\n",
       "   'learning_rate': 1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.75,\n",
       "   'learning_rate': 1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.75,\n",
       "   'learning_rate': 1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.75,\n",
       "   'learning_rate': 1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.75,\n",
       "   'learning_rate': 1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.75,\n",
       "   'learning_rate': 1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.75,\n",
       "   'learning_rate': 1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.75,\n",
       "   'learning_rate': 1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.75,\n",
       "   'learning_rate': 1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 1,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 1,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 1,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 1,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 1,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 1,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 1,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 1,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 1,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 1,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 1,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 1,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 1,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 1,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 1,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 1,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 1,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 1,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 1,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 1,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 1,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 1,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 1,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 1,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 1,\n",
       "   'learning_rate': 0.75,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 1,\n",
       "   'learning_rate': 0.75,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 1,\n",
       "   'learning_rate': 0.75,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 1,\n",
       "   'learning_rate': 0.75,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 1,\n",
       "   'learning_rate': 0.75,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 1,\n",
       "   'learning_rate': 0.75,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 1,\n",
       "   'learning_rate': 0.75,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 1,\n",
       "   'learning_rate': 0.75,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 1,\n",
       "   'learning_rate': 0.75,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 1,\n",
       "   'learning_rate': 0.75,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 1,\n",
       "   'learning_rate': 0.75,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 1,\n",
       "   'learning_rate': 0.75,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 1,\n",
       "   'learning_rate': 1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 1,\n",
       "   'learning_rate': 1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 1,\n",
       "   'learning_rate': 1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 1,\n",
       "   'learning_rate': 1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 1,\n",
       "   'learning_rate': 1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 1,\n",
       "   'learning_rate': 1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 1,\n",
       "   'learning_rate': 1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 1,\n",
       "   'learning_rate': 1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 1,\n",
       "   'learning_rate': 1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 1,\n",
       "   'learning_rate': 1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 1,\n",
       "   'learning_rate': 1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 1,\n",
       "   'learning_rate': 1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5}],\n",
       " 'split0_test_score': array([0.84313725, 0.82352941, 0.80392157, 0.82352941, 0.8       ,\n",
       "        0.78095238, 0.84615385, 0.8       , 0.8       , 0.81553398,\n",
       "        0.8       , 0.8       , 0.83168317, 0.85714286, 0.83495146,\n",
       "        0.80808081, 0.8411215 , 0.83495146, 0.77669903, 0.85714286,\n",
       "        0.80392157, 0.78431373, 0.8490566 , 0.81553398, 0.85436893,\n",
       "        0.81904762, 0.82352941, 0.79207921, 0.81132075, 0.83495146,\n",
       "        0.79207921, 0.85714286, 0.81553398, 0.8       , 0.83018868,\n",
       "        0.82      , 0.82828283, 0.87037037, 0.82      , 0.79207921,\n",
       "        0.87619048, 0.84615385, 0.76      , 0.82352941, 0.8       ,\n",
       "        0.76923077, 0.8490566 , 0.81553398, 0.84313725, 0.85148515,\n",
       "        0.81188119, 0.83495146, 0.85714286, 0.82692308, 0.81904762,\n",
       "        0.84615385, 0.80392157, 0.82692308, 0.85436893, 0.80392157,\n",
       "        0.84615385, 0.84615385, 0.83809524, 0.81553398, 0.82242991,\n",
       "        0.81553398, 0.78846154, 0.79207921, 0.79207921, 0.78504673,\n",
       "        0.83168317, 0.80392157, 0.83495146, 0.85714286, 0.84      ,\n",
       "        0.77669903, 0.83809524, 0.83168317, 0.78846154, 0.82242991,\n",
       "        0.81553398, 0.80373832, 0.80769231, 0.77669903, 0.81132075,\n",
       "        0.8411215 , 0.82692308, 0.80373832, 0.80769231, 0.77669903,\n",
       "        0.79245283, 0.77227723, 0.80392157, 0.76923077, 0.8       ,\n",
       "        0.7961165 , 0.85148515, 0.84615385, 0.84615385, 0.8627451 ,\n",
       "        0.86538462, 0.85148515, 0.82242991, 0.85436893, 0.84313725,\n",
       "        0.81553398, 0.8627451 , 0.84313725, 0.84615385, 0.82692308,\n",
       "        0.85436893, 0.80392157, 0.82692308, 0.82352941, 0.8       ,\n",
       "        0.83809524, 0.81553398, 0.82242991, 0.80392157, 0.84      ,\n",
       "        0.83495146, 0.80392157, 0.83168317, 0.77669903, 0.8       ,\n",
       "        0.81481481, 0.80733945, 0.79245283, 0.8       , 0.78095238,\n",
       "        0.8411215 , 0.7962963 , 0.78431373, 0.82352941, 0.80392157,\n",
       "        0.81904762, 0.78846154, 0.83495146, 0.81132075, 0.80769231,\n",
       "        0.78787879, 0.80392157, 0.76470588, 0.82692308, 0.84313725,\n",
       "        0.83495146, 0.8627451 , 0.84313725, 0.87378641, 0.87378641,\n",
       "        0.84313725, 0.84      , 0.87378641, 0.82692308, 0.86      ,\n",
       "        0.87378641, 0.82692308, 0.86538462, 0.8627451 , 0.86538462,\n",
       "        0.88461538, 0.83495146, 0.80769231, 0.8490566 , 0.84313725,\n",
       "        0.81904762, 0.85714286, 0.87128713, 0.84313725, 0.87850467,\n",
       "        0.83809524, 0.81132075, 0.82242991, 0.82692308, 0.80769231,\n",
       "        0.82692308, 0.83495146, 0.78504673, 0.8411215 , 0.83495146,\n",
       "        0.83809524, 0.8411215 , 0.85436893, 0.78846154, 0.80769231,\n",
       "        0.78      , 0.7961165 , 0.84313725, 0.83018868, 0.77669903,\n",
       "        0.79245283, 0.83018868]),\n",
       " 'split1_test_score': array([0.68686869, 0.70707071, 0.66666667, 0.7       , 0.68      ,\n",
       "        0.68686869, 0.70588235, 0.69387755, 0.70707071, 0.70707071,\n",
       "        0.70707071, 0.71287129, 0.74      , 0.69306931, 0.68627451,\n",
       "        0.72      , 0.69306931, 0.72      , 0.69306931, 0.73076923,\n",
       "        0.70707071, 0.7254902 , 0.7254902 , 0.7       , 0.75247525,\n",
       "        0.7254902 , 0.7       , 0.69230769, 0.7       , 0.73267327,\n",
       "        0.71698113, 0.71287129, 0.72916667, 0.71153846, 0.7184466 ,\n",
       "        0.72      , 0.74      , 0.69902913, 0.7184466 , 0.69902913,\n",
       "        0.73267327, 0.74      , 0.68571429, 0.68627451, 0.74747475,\n",
       "        0.69230769, 0.66666667, 0.75247525, 0.71287129, 0.71287129,\n",
       "        0.7254902 , 0.72727273, 0.7       , 0.7184466 , 0.72      ,\n",
       "        0.71287129, 0.69306931, 0.73267327, 0.71287129, 0.70588235,\n",
       "        0.73267327, 0.74      , 0.7254902 , 0.71028037, 0.73469388,\n",
       "        0.70588235, 0.71698113, 0.72      , 0.67326733, 0.71559633,\n",
       "        0.73786408, 0.70588235, 0.74285714, 0.73469388, 0.70707071,\n",
       "        0.72222222, 0.73786408, 0.74509804, 0.7027027 , 0.73267327,\n",
       "        0.71287129, 0.71559633, 0.73076923, 0.72164948, 0.71287129,\n",
       "        0.67961165, 0.73076923, 0.72222222, 0.70588235, 0.72164948,\n",
       "        0.72727273, 0.72380952, 0.7       , 0.71559633, 0.73584906,\n",
       "        0.7       , 0.74      , 0.72      , 0.70588235, 0.72727273,\n",
       "        0.74      , 0.7254902 , 0.72      , 0.75510204, 0.73267327,\n",
       "        0.77669903, 0.73469388, 0.73267327, 0.73076923, 0.73267327,\n",
       "        0.73469388, 0.7037037 , 0.75728155, 0.77227723, 0.75925926,\n",
       "        0.77227723, 0.7254902 , 0.72380952, 0.76190476, 0.7254902 ,\n",
       "        0.72      , 0.73267327, 0.73786408, 0.7037037 , 0.75728155,\n",
       "        0.69306931, 0.73394495, 0.76923077, 0.76470588, 0.71698113,\n",
       "        0.78095238, 0.72727273, 0.7254902 , 0.72380952, 0.73786408,\n",
       "        0.71559633, 0.75      , 0.73786408, 0.73394495, 0.75728155,\n",
       "        0.68      , 0.71028037, 0.71028037, 0.7184466 , 0.74226804,\n",
       "        0.72727273, 0.71287129, 0.76470588, 0.70833333, 0.73469388,\n",
       "        0.76470588, 0.72727273, 0.72727273, 0.76470588, 0.76767677,\n",
       "        0.76      , 0.73267327, 0.70103093, 0.74509804, 0.74509804,\n",
       "        0.76923077, 0.77669903, 0.7254902 , 0.76190476, 0.77227723,\n",
       "        0.72380952, 0.75247525, 0.76923077, 0.73267327, 0.74509804,\n",
       "        0.74      , 0.7254902 , 0.73267327, 0.73786408, 0.73786408,\n",
       "        0.77358491, 0.79207921, 0.68627451, 0.76470588, 0.78846154,\n",
       "        0.75      , 0.73469388, 0.73267327, 0.76190476, 0.80392157,\n",
       "        0.7184466 , 0.74      , 0.77358491, 0.69306931, 0.7184466 ,\n",
       "        0.78095238, 0.69306931]),\n",
       " 'split2_test_score': array([0.75247525, 0.78431373, 0.76470588, 0.74285714, 0.74285714,\n",
       "        0.75247525, 0.74285714, 0.76923077, 0.73786408, 0.73394495,\n",
       "        0.75471698, 0.75      , 0.73076923, 0.74285714, 0.69811321,\n",
       "        0.74074074, 0.69811321, 0.72380952, 0.71559633, 0.71028037,\n",
       "        0.7047619 , 0.7047619 , 0.7027027 , 0.69811321, 0.76470588,\n",
       "        0.72897196, 0.72897196, 0.71559633, 0.68468468, 0.69811321,\n",
       "        0.70909091, 0.66071429, 0.69158879, 0.69642857, 0.69090909,\n",
       "        0.69158879, 0.72380952, 0.73394495, 0.7047619 , 0.71028037,\n",
       "        0.68518519, 0.69230769, 0.69724771, 0.70909091, 0.68518519,\n",
       "        0.73873874, 0.70909091, 0.7037037 , 0.76190476, 0.77669903,\n",
       "        0.76470588, 0.74285714, 0.75      , 0.75728155, 0.75      ,\n",
       "        0.75      , 0.74285714, 0.75471698, 0.75      , 0.75728155,\n",
       "        0.72380952, 0.72897196, 0.74766355, 0.73394495, 0.72727273,\n",
       "        0.71028037, 0.74766355, 0.70909091, 0.72222222, 0.71428571,\n",
       "        0.72222222, 0.71028037, 0.71559633, 0.72222222, 0.72222222,\n",
       "        0.70909091, 0.7027027 , 0.7037037 , 0.74336283, 0.72072072,\n",
       "        0.69090909, 0.72727273, 0.71428571, 0.71028037, 0.70909091,\n",
       "        0.71028037, 0.72222222, 0.69642857, 0.71428571, 0.7047619 ,\n",
       "        0.74766355, 0.69090909, 0.72072072, 0.73214286, 0.68965517,\n",
       "        0.7037037 , 0.74766355, 0.75      , 0.76923077, 0.75      ,\n",
       "        0.75728155, 0.73584906, 0.73584906, 0.73584906, 0.72897196,\n",
       "        0.72897196, 0.73584906, 0.72897196, 0.73584906, 0.71028037,\n",
       "        0.72380952, 0.72897196, 0.69811321, 0.72222222, 0.72222222,\n",
       "        0.7037037 , 0.72222222, 0.72222222, 0.71428571, 0.71559633,\n",
       "        0.74766355, 0.72897196, 0.72222222, 0.75675676, 0.72727273,\n",
       "        0.70909091, 0.7079646 , 0.7047619 , 0.68468468, 0.74074074,\n",
       "        0.72222222, 0.69090909, 0.74766355, 0.68468468, 0.69090909,\n",
       "        0.74545455, 0.70175439, 0.7037037 , 0.73873874, 0.72072072,\n",
       "        0.73214286, 0.73394495, 0.70175439, 0.69026549, 0.73584906,\n",
       "        0.74285714, 0.76190476, 0.73584906, 0.73584906, 0.74074074,\n",
       "        0.76190476, 0.72897196, 0.74285714, 0.75471698, 0.72897196,\n",
       "        0.73584906, 0.73394495, 0.74766355, 0.71698113, 0.72380952,\n",
       "        0.72727273, 0.72222222, 0.73076923, 0.73394495, 0.69724771,\n",
       "        0.77777778, 0.73394495, 0.67924528, 0.74074074, 0.74074074,\n",
       "        0.69724771, 0.74336283, 0.71559633, 0.71559633, 0.74074074,\n",
       "        0.72222222, 0.70909091, 0.77777778, 0.75229358, 0.70909091,\n",
       "        0.75925926, 0.73584906, 0.72727273, 0.72072072, 0.75229358,\n",
       "        0.72222222, 0.72897196, 0.73873874, 0.7037037 , 0.75471698,\n",
       "        0.71559633, 0.7037037 ]),\n",
       " 'split3_test_score': array([0.80373832, 0.80373832, 0.77358491, 0.82242991, 0.82568807,\n",
       "        0.83333333, 0.82242991, 0.82568807, 0.83333333, 0.80373832,\n",
       "        0.80733945, 0.83333333, 0.77669903, 0.79245283, 0.8       ,\n",
       "        0.76190476, 0.77358491, 0.76470588, 0.73786408, 0.77358491,\n",
       "        0.77669903, 0.76923077, 0.75      , 0.73267327, 0.76470588,\n",
       "        0.74      , 0.77358491, 0.72897196, 0.73584906, 0.7254902 ,\n",
       "        0.73786408, 0.75728155, 0.7254902 , 0.75471698, 0.75471698,\n",
       "        0.75247525, 0.73786408, 0.76470588, 0.74      , 0.68571429,\n",
       "        0.7254902 , 0.76190476, 0.75728155, 0.73584906, 0.70588235,\n",
       "        0.74      , 0.65306122, 0.75      , 0.80769231, 0.8       ,\n",
       "        0.77669903, 0.78846154, 0.80769231, 0.79207921, 0.77358491,\n",
       "        0.80769231, 0.80392157, 0.77669903, 0.8       , 0.80392157,\n",
       "        0.78431373, 0.7961165 , 0.77669903, 0.76635514, 0.76      ,\n",
       "        0.76923077, 0.75471698, 0.77358491, 0.76470588, 0.78846154,\n",
       "        0.76190476, 0.76470588, 0.78095238, 0.76923077, 0.7961165 ,\n",
       "        0.76190476, 0.75      , 0.76190476, 0.78095238, 0.76190476,\n",
       "        0.78095238, 0.76923077, 0.77669903, 0.76923077, 0.74509804,\n",
       "        0.74509804, 0.75      , 0.75471698, 0.77669903, 0.76923077,\n",
       "        0.72380952, 0.76190476, 0.78095238, 0.73786408, 0.76190476,\n",
       "        0.76923077, 0.80769231, 0.80373832, 0.79245283, 0.80392157,\n",
       "        0.81553398, 0.7961165 , 0.76635514, 0.80769231, 0.80392157,\n",
       "        0.77358491, 0.80769231, 0.79207921, 0.75471698, 0.78095238,\n",
       "        0.80392157, 0.75728155, 0.76190476, 0.78846154, 0.76190476,\n",
       "        0.7706422 , 0.77358491, 0.74285714, 0.79245283, 0.76635514,\n",
       "        0.75      , 0.76923077, 0.74509804, 0.73584906, 0.78846154,\n",
       "        0.77777778, 0.77358491, 0.78095238, 0.78504673, 0.77777778,\n",
       "        0.78899083, 0.77477477, 0.75      , 0.79245283, 0.76470588,\n",
       "        0.74509804, 0.75675676, 0.7706422 , 0.76635514, 0.82242991,\n",
       "        0.75471698, 0.76923077, 0.75229358, 0.75675676, 0.80769231,\n",
       "        0.80769231, 0.80769231, 0.81553398, 0.81553398, 0.7961165 ,\n",
       "        0.80769231, 0.82352941, 0.7961165 , 0.79245283, 0.80769231,\n",
       "        0.80769231, 0.75471698, 0.80769231, 0.81132075, 0.77358491,\n",
       "        0.76190476, 0.75471698, 0.78846154, 0.76635514, 0.74509804,\n",
       "        0.76190476, 0.78095238, 0.76923077, 0.77669903, 0.8       ,\n",
       "        0.78431373, 0.77227723, 0.76190476, 0.73076923, 0.76635514,\n",
       "        0.76190476, 0.75      , 0.78846154, 0.77669903, 0.7961165 ,\n",
       "        0.76767677, 0.78431373, 0.77777778, 0.76470588, 0.75728155,\n",
       "        0.75471698, 0.74285714, 0.76923077, 0.76635514, 0.76190476,\n",
       "        0.79245283, 0.72380952]),\n",
       " 'split4_test_score': array([0.72727273, 0.72      , 0.72727273, 0.73786408, 0.7254902 ,\n",
       "        0.72      , 0.73786408, 0.73267327, 0.72727273, 0.74      ,\n",
       "        0.74747475, 0.72      , 0.75247525, 0.74      , 0.72916667,\n",
       "        0.77227723, 0.71428571, 0.74226804, 0.77083333, 0.73267327,\n",
       "        0.72916667, 0.74226804, 0.73469388, 0.72727273, 0.77669903,\n",
       "        0.72727273, 0.75247525, 0.75      , 0.72727273, 0.78      ,\n",
       "        0.76      , 0.74226804, 0.77227723, 0.76      , 0.72164948,\n",
       "        0.77227723, 0.73267327, 0.72727273, 0.73684211, 0.7254902 ,\n",
       "        0.68627451, 0.76767677, 0.74      , 0.74747475, 0.76470588,\n",
       "        0.69902913, 0.72      , 0.77358491, 0.70212766, 0.68686869,\n",
       "        0.69473684, 0.7755102 , 0.76767677, 0.75      , 0.79207921,\n",
       "        0.7628866 , 0.80412371, 0.8       , 0.74226804, 0.7628866 ,\n",
       "        0.77227723, 0.75      , 0.78787879, 0.74226804, 0.7628866 ,\n",
       "        0.7032967 , 0.72727273, 0.76      , 0.75      , 0.70588235,\n",
       "        0.74747475, 0.75      , 0.7628866 , 0.7628866 , 0.75789474,\n",
       "        0.73469388, 0.68627451, 0.73469388, 0.76767677, 0.78350515,\n",
       "        0.74      , 0.71287129, 0.78431373, 0.72727273, 0.75      ,\n",
       "        0.72727273, 0.73333333, 0.71428571, 0.70588235, 0.72916667,\n",
       "        0.75510204, 0.68041237, 0.69902913, 0.72      , 0.7       ,\n",
       "        0.72916667, 0.71428571, 0.6875    , 0.70833333, 0.79591837,\n",
       "        0.75789474, 0.74468085, 0.80808081, 0.78350515, 0.77083333,\n",
       "        0.80808081, 0.75      , 0.74468085, 0.76767677, 0.73469388,\n",
       "        0.78      , 0.74747475, 0.76767677, 0.76      , 0.74      ,\n",
       "        0.74      , 0.75510204, 0.76470588, 0.76767677, 0.76470588,\n",
       "        0.76      , 0.72164948, 0.78787879, 0.73469388, 0.73267327,\n",
       "        0.73267327, 0.71153846, 0.75247525, 0.80412371, 0.7184466 ,\n",
       "        0.75247525, 0.74226804, 0.7254902 , 0.7       , 0.73267327,\n",
       "        0.68686869, 0.76190476, 0.7311828 , 0.74509804, 0.75728155,\n",
       "        0.75247525, 0.76      , 0.73786408, 0.72      , 0.74226804,\n",
       "        0.70833333, 0.7311828 , 0.78350515, 0.75789474, 0.75789474,\n",
       "        0.8       , 0.77083333, 0.77083333, 0.78431373, 0.75      ,\n",
       "        0.77083333, 0.78350515, 0.72164948, 0.78350515, 0.74509804,\n",
       "        0.72916667, 0.70103093, 0.75      , 0.73267327, 0.73267327,\n",
       "        0.75      , 0.74509804, 0.73267327, 0.74747475, 0.7628866 ,\n",
       "        0.77083333, 0.75247525, 0.76470588, 0.7254902 , 0.76923077,\n",
       "        0.71153846, 0.74747475, 0.74      , 0.70588235, 0.74747475,\n",
       "        0.69473684, 0.7254902 , 0.7755102 , 0.7184466 , 0.7628866 ,\n",
       "        0.72727273, 0.72727273, 0.69230769, 0.70588235, 0.7184466 ,\n",
       "        0.72380952, 0.70588235]),\n",
       " 'mean_test_score': array([0.76269845, 0.76773043, 0.74723035, 0.76533611, 0.75480708,\n",
       "        0.75472593, 0.77103747, 0.76429393, 0.76110817, 0.76005759,\n",
       "        0.76332038, 0.76324092, 0.76632534, 0.76510443, 0.74970117,\n",
       "        0.76060071, 0.74403493, 0.75714698, 0.73881242, 0.76089013,\n",
       "        0.74432398, 0.74521293, 0.75238868, 0.73471864, 0.78259099,\n",
       "        0.7481565 , 0.75571231, 0.73579104, 0.73182544, 0.75424563,\n",
       "        0.74320307, 0.7460556 , 0.74681137, 0.7445368 , 0.74318217,\n",
       "        0.75126825, 0.75252594, 0.75906461, 0.74401012, 0.72251864,\n",
       "        0.74116273, 0.76160861, 0.72804871, 0.74044373, 0.74064963,\n",
       "        0.72786127, 0.71957508, 0.75905957, 0.76554665, 0.76558483,\n",
       "        0.75470263, 0.77381061, 0.77650239, 0.76894609, 0.77094235,\n",
       "        0.77592081, 0.76957866, 0.77820247, 0.77190165, 0.76677873,\n",
       "        0.77184552, 0.77224846, 0.77516536, 0.7536765 , 0.76145662,\n",
       "        0.74084484, 0.74701919, 0.750951  , 0.74045493, 0.74185453,\n",
       "        0.7602298 , 0.74695804, 0.76744878, 0.76923526, 0.76466083,\n",
       "        0.74092216, 0.74298731, 0.75541671, 0.75663124, 0.76424676,\n",
       "        0.74805335, 0.74574189, 0.762752  , 0.74102648, 0.7456762 ,\n",
       "        0.74067686, 0.75264957, 0.73827836, 0.74208835, 0.74030157,\n",
       "        0.74926013, 0.7258626 , 0.74092476, 0.73496681, 0.7374818 ,\n",
       "        0.73964353, 0.77222534, 0.76147843, 0.76441063, 0.78797155,\n",
       "        0.78721898, 0.77072435, 0.77054298, 0.7873035 , 0.77590748,\n",
       "        0.78057414, 0.77819607, 0.76830851, 0.76703318, 0.7571046 ,\n",
       "        0.77935878, 0.74827071, 0.76237987, 0.77329808, 0.75667725,\n",
       "        0.76494367, 0.75838667, 0.75520494, 0.76804833, 0.76242951,\n",
       "        0.762523  , 0.75128941, 0.76494926, 0.74154048, 0.76113782,\n",
       "        0.74548522, 0.74687447, 0.75997463, 0.7677122 , 0.74697973,\n",
       "        0.77715243, 0.74630419, 0.74659153, 0.74489529, 0.74601478,\n",
       "        0.74241304, 0.75177549, 0.75566885, 0.75909153, 0.77308121,\n",
       "        0.74144277, 0.75547553, 0.73337966, 0.74247838, 0.77424294,\n",
       "        0.76422139, 0.77527925, 0.78854627, 0.7782795 , 0.78064645,\n",
       "        0.79548804, 0.77812149, 0.78217322, 0.7846225 , 0.78286821,\n",
       "        0.78963222, 0.76635269, 0.76868418, 0.78393004, 0.77059502,\n",
       "        0.77443806, 0.75792412, 0.76048265, 0.76878695, 0.7580867 ,\n",
       "        0.76650794, 0.7739227 , 0.76433344, 0.76814501, 0.78544601,\n",
       "        0.766098  , 0.76098525, 0.75946203, 0.74732858, 0.76437661,\n",
       "        0.75923469, 0.76671926, 0.75551211, 0.76814047, 0.77521903,\n",
       "        0.76195362, 0.76429367, 0.77352058, 0.7508479 , 0.77681512,\n",
       "        0.74053171, 0.74704367, 0.76339987, 0.73983984, 0.7460428 ,\n",
       "        0.76105278, 0.73133071]),\n",
       " 'std_test_score': array([0.05525273, 0.04613653, 0.04712253, 0.04934569, 0.0522779 ,\n",
       "        0.0503637 , 0.0537093 , 0.04696141, 0.04761139, 0.0421365 ,\n",
       "        0.03680272, 0.04657548, 0.03613404, 0.05573731, 0.0581438 ,\n",
       "        0.02978334, 0.05637432, 0.04200795, 0.03191568, 0.05232696,\n",
       "        0.03945307, 0.02877385, 0.05070646, 0.04274596, 0.03669741,\n",
       "        0.03580508, 0.04182816, 0.03380941, 0.04379847, 0.04820749,\n",
       "        0.03016869, 0.06459889, 0.04286437, 0.03693834, 0.04798159,\n",
       "        0.04405957, 0.03828769, 0.05944203, 0.0400895 , 0.03715803,\n",
       "        0.07028   , 0.04990863, 0.03085185, 0.04667136, 0.0410678 ,\n",
       "        0.02853928, 0.06942472, 0.03630439, 0.05404902, 0.05943356,\n",
       "        0.04074937, 0.0376288 , 0.05311983, 0.0372604 , 0.03406339,\n",
       "        0.04638778, 0.04498878, 0.0330925 , 0.04986539, 0.03625712,\n",
       "        0.04363323, 0.04344998, 0.03833569, 0.03574858, 0.03348639,\n",
       "        0.04461088, 0.02477639, 0.03160993, 0.04047441, 0.03682746,\n",
       "        0.03798916, 0.03633606, 0.04013277, 0.04726814, 0.04863926,\n",
       "        0.02498784, 0.05283963, 0.04258829, 0.03101944, 0.03647753,\n",
       "        0.0451754 , 0.03534305, 0.03478775, 0.02675059, 0.03672297,\n",
       "        0.05466489, 0.03821325, 0.03778911, 0.04218178, 0.0279164 ,\n",
       "        0.02463114, 0.03672885, 0.04337993, 0.01891882, 0.04049677,\n",
       "        0.03751705, 0.05007749, 0.05706285, 0.05303164, 0.04697834,\n",
       "        0.0466849 , 0.04713331, 0.03969166, 0.04150276, 0.04336478,\n",
       "        0.03067722, 0.04999859, 0.0436804 , 0.0417126 , 0.04179707,\n",
       "        0.04756246, 0.03327914, 0.04086638, 0.03328586, 0.02598513,\n",
       "        0.0442943 , 0.03434424, 0.03697422, 0.0310186 , 0.04380784,\n",
       "        0.03856963, 0.03103286, 0.03983078, 0.02440717, 0.02908196,\n",
       "        0.04491219, 0.03820797, 0.03061131, 0.04375247, 0.0277692 ,\n",
       "        0.03968103, 0.03676109, 0.02156972, 0.05390836, 0.03737277,\n",
       "        0.04403097, 0.02820831, 0.04500419, 0.02836567, 0.03707857,\n",
       "        0.03554132, 0.03182095, 0.02405375, 0.04720968, 0.04332243,\n",
       "        0.04865711, 0.05425681, 0.03762788, 0.0593596 , 0.05125645,\n",
       "        0.03005038, 0.04678722, 0.05152825, 0.02506695, 0.04643588,\n",
       "        0.04802539, 0.03544837, 0.06017259, 0.05089661, 0.04996615,\n",
       "        0.05761477, 0.04649184, 0.03233826, 0.04246094, 0.04888947,\n",
       "        0.03163632, 0.04441641, 0.06281627, 0.04033238, 0.05100427,\n",
       "        0.04681732, 0.02933953, 0.03644244, 0.04045472, 0.02516067,\n",
       "        0.04108437, 0.04305734, 0.03869933, 0.04367716, 0.04318033,\n",
       "        0.04582258, 0.04357584, 0.0455322 , 0.02715354, 0.02393754,\n",
       "        0.02348513, 0.02526959, 0.04931137, 0.05198536, 0.02362096,\n",
       "        0.03412116, 0.05040689]),\n",
       " 'rank_test_score': array([ 81,  53, 136,  65, 117, 118,  39,  73,  90,  97,  78,  79,  61,\n",
       "         66, 130,  94, 155, 107, 179,  93, 154, 151, 124, 184,  11, 133,\n",
       "        111, 182, 186, 120, 157, 145, 142, 153, 158, 127, 123, 102, 156,\n",
       "        191, 166,  86, 188, 175, 172, 189, 192, 103,  64,  63, 119,  31,\n",
       "         22,  46,  40,  23,  44,  17,  37,  57,  38,  35,  27, 121,  88,\n",
       "        170, 138, 128, 174, 163,  96, 140,  55,  45,  69, 169, 159, 115,\n",
       "        110,  75, 134, 148,  80, 167, 149, 171, 122, 180, 162, 176, 131,\n",
       "        190, 168, 183, 181, 178,  36,  87,  70,   4,   6,  41,  43,   5,\n",
       "         24,  14,  18,  49,  56, 108,  15, 132,  84,  33, 109,  68, 104,\n",
       "        116,  52,  83,  82, 126,  67, 164,  89, 150, 141,  98,  54, 139,\n",
       "         20, 144, 143, 152, 147, 161, 125, 112, 101,  34, 165, 114, 185,\n",
       "        160,  29,  76,  25,   3,  16,  13,   1,  19,  12,   8,  10,   2,\n",
       "         60,  48,   9,  42,  28, 106,  95,  47, 105,  59,  30,  72,  50,\n",
       "          7,  62,  92,  99, 135,  71, 100,  58, 113,  51,  26,  85,  74,\n",
       "         32, 129,  21, 173, 137,  77, 177, 146,  91, 187], dtype=int32)}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T20:05:08.012781Z",
     "start_time": "2019-06-25T20:05:08.002972Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 1,\n",
       " 'learning_rate': 0.1,\n",
       " 'max_depth': 6,\n",
       " 'min_child_weight': 1}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T20:05:08.025138Z",
     "start_time": "2019-06-25T20:05:08.017697Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7954880413703943"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T20:05:08.051389Z",
     "start_time": "2019-06-25T20:05:08.029347Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.771300\n",
      "F1: 0.657718\n"
     ]
    }
   ],
   "source": [
    "preds = gsearch1.best_estimator_.predict(X_test)\n",
    "\n",
    "\n",
    "test_f1 = f1_score(y_test, preds)\n",
    "test_acc = accuracy_score(y_test, preds)\n",
    "\n",
    "print(\"Accuracy: %f\" % (test_acc))\n",
    "print(\"F1: %f\" % (test_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T20:05:08.064250Z",
     "start_time": "2019-06-25T20:05:08.056111Z"
    }
   },
   "outputs": [],
   "source": [
    "from xgboost import plot_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T20:05:08.414592Z",
     "start_time": "2019-06-25T20:05:08.068799Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x11f8ab630>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# plot feature importance\n",
    "plot_importance(alg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T20:05:08.427033Z",
     "start_time": "2019-06-25T20:05:08.416812Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'S': 40,\n",
       " 'Q': 20,\n",
       " 'Fare': 161,\n",
       " 'Parch': 64,\n",
       " 'Pclass': 51,\n",
       " 'SibSp': 82,\n",
       " 'Age': 121,\n",
       " 'male': 32,\n",
       " 'youngin': 32}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alg.get_booster().get_fscore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T20:05:08.439014Z",
     "start_time": "2019-06-25T20:05:08.429319Z"
    }
   },
   "outputs": [],
   "source": [
    "xg_clf = xgb.train(params=params, dtrain=data_dmatrix, num_boost_round=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T20:05:08.449928Z",
     "start_time": "2019-06-25T20:05:08.445605Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T20:05:08.461949Z",
     "start_time": "2019-06-25T20:05:08.453438Z"
    }
   },
   "outputs": [],
   "source": [
    "# pickle list object\n",
    " \n",
    "model_pickle_path = 'xg_boost_model.pkl'\n",
    "\n",
    "# Create an variable to pickle and open it in write mode\n",
    "model_pickle = open(model_pickle_path, 'wb')\n",
    "pickle.dump(gsearch1.best_estimator_, model_pickle)\n",
    "model_pickle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T20:05:08.476258Z",
     "start_time": "2019-06-25T20:05:08.465173Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded XGboost model ::  XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "              max_depth=6, min_child_weight=1, missing=nan, n_estimators=140,\n",
      "              n_jobs=1, nthread=4, objective='binary:logistic', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=27,\n",
      "              silent=True, subsample=0.8)\n"
     ]
    }
   ],
   "source": [
    "# Loading the saved XGboost model pickle\n",
    "xgboost_model_pkl = open(model_pickle_path, 'rb')\n",
    "xgboost_model = pickle.load(xgboost_model_pkl)\n",
    "print(\"Loaded XGboost model :: \", xgboost_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T20:05:08.486480Z",
     "start_time": "2019-06-25T20:05:08.478981Z"
    }
   },
   "outputs": [],
   "source": [
    "gridsearch_params = [\n",
    "    (max_depth, min_child_weight)\n",
    "    for max_depth in range(9,12)\n",
    "    for min_child_weight in range(5,8)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T20:05:08.498165Z",
     "start_time": "2019-06-25T20:05:08.489463Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9, 5), (9, 6), (9, 7), (10, 5), (10, 6), (10, 7), (11, 5), (11, 6), (11, 7)]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
